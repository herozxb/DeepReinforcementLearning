{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Num\tObservation\tMin\tMax\n",
    "0\tCart Position\t-2.4\t2.4\n",
    "1\tCart Velocity\t-Inf\tInf\n",
    "2\tPole Angle\t~ -41.8°\t~ 41.8°\n",
    "3\tPole Velocity At Tip\t-Inf\tInf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-28 16:20:30,697] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "#创建CartPole问题的环境env\n",
    "env.reset()\n",
    "#初始化环境\n",
    "random_episodes = 0\n",
    "reward_sum = 0#奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game over,Reward for this episode was: 18.0\n",
      "game over,Reward for this episode was: 16.0\n",
      "game over,Reward for this episode was: 40.0\n",
      "game over,Reward for this episode was: 11.0\n",
      "game over,Reward for this episode was: 17.0\n",
      "game over,Reward for this episode was: 14.0\n",
      "game over,Reward for this episode was: 29.0\n",
      "game over,Reward for this episode was: 16.0\n",
      "game over,Reward for this episode was: 58.0\n",
      "game over,Reward for this episode was: 13.0\n",
      "随机测试结束\n"
     ]
    }
   ],
   "source": [
    "random_episodes = 0\n",
    "reward_sum = 0#奖励\n",
    "while random_episodes < 10:\n",
    "    env.render()#将CartPole问题的图像渲染出来\n",
    "\n",
    "    observation, reward, done, _ = env.step(np.random.randint(0, 2))\n",
    "    #使用np.random.randint(0, 2)产生随机的Action\n",
    "    #然后使用env.step()执行随机的Action,并获取返回值\n",
    "    #如果done标记为True,则表示这次试验结束，即倾角超过15度或者偏离中心过远导致任务失败\n",
    "\n",
    "    reward_sum += reward\n",
    "    if done:#如果试验结束\n",
    "        random_episodes += 1\n",
    "        print(\"game over,Reward for this episode was:\", reward_sum)\n",
    "        #输出这次试验累计的奖励\n",
    "        reward_sum = 0 #奖励重新置为0\n",
    "        env.reset()#重启环境\n",
    "\n",
    "\n",
    "print (\"随机测试结束\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 超参数\n",
    "H = 50  # 隐含的节点数\n",
    "batch_size = 25  #\n",
    "learning_rate = 1e-1  # 学习率\n",
    "gamma = 0.99  # Reward的discount比例设为0.99,该值必须小于1.\n",
    "#防止Reward被无损耗地不断累加导致发散，这样也能区分当前Reward和未来的Reward的价值\n",
    "#当前Action直接带来的Reward不需要discount，而未来的Reward因存在不确定性，所以需要discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-D = 4  # 环境信息observation的维度D为4\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#策略网络的具体结构。\n",
    "#该网络将接受observation作为信息输入，最后输出一个概率值用以选择Action\n",
    "#这里只有两个Action，向左施加力或者向右施加力，因此可以通过一个概率值决定\n",
    "\n",
    "observations = tf.placeholder(tf.float32, [None, D], name=\"input_x\")\n",
    "#创建输入信息observations的placeholder其维度为D\n",
    "\n",
    "#使用tf.contrib.layers.xavier_initializer方法初始化隐含层的权重W1，其维度为[D,H]\n",
    "W1 = tf.get_variable(\"W1\", shape=[D, H],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "layer1 = tf.nn.relu(tf.matmul(observations, W1))\n",
    "#接着使用tf.matmul将环境信息observations乘上W1再使用relu激活函数处理得到隐含层的输出layer1\n",
    "\n",
    "#使用tf.contrib.layers.xavier_initializer方法初始化隐含层的权重W2，其维度为[H,1]\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1, W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "#将隐含层输出layer1乘以W2后，使用Sigmoid激活函数处理得到最后的输出概率\n",
    "\n",
    "\n",
    "# From here we define the parts of the network needed for learning a good policy.\n",
    "tvars = tf.trainable_variables()#获取策略网络中全部可训练的参数tvars\n",
    "input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "#定义人工设置的虚拟label的占位符input_y\n",
    "#以及每个Action的潜在价值的占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "# We don't just apply gradeients after every episode in order to account for noise in the reward signal.\n",
    "#模型的优化器使用Adam算法\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)  # Our optimizer\n",
    "W1Grad = tf.placeholder(tf.float32, name=\"batch_grad1\")  \n",
    "# Placeholders to send the final gradients through when we update.\n",
    "W2Grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "#我们分别设置两层神经网络参数的梯度的placeholder\n",
    "\n",
    "batchGrad = [W1Grad, W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad, tvars))\n",
    "#并使用adam.apply_gradients定义我们更新模型参数的操作updateGrads\n",
    "#之后计算参数的梯度，当积累到一定样本量的梯度，就传入W1Grad和W2Grad，并执行updateGrads更新模型参数\n",
    "#注意：\n",
    "#深度强化学习的训练和其他神经网络一样，也使用batch training的方式。\n",
    "#我们不逐个样本的更新参数，而是累计到一个batch_size的样本的梯度在更新参数\n",
    "#防止单一样本随机扰动的噪声对模型带来不良影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用来估算每一个Action对应的潜在价值discount_r\n",
    "#因为CartPole问题中每次获得的Reward都和前面的Action有关，属于delayed reward\n",
    "#因此需要比较精确地衡量每一个Action实际带来的价值，不能只看当前这一步的Reward，而要考虑后面的Delayed Reward\n",
    "#哪些能让Pole长时间保持在空中竖直的Action应该拥有较大的值，而哪些最终导致pole倾倒的Action，应该拥有较小的期望价值。\n",
    "#我们判断越靠后的Aciton的期望价值越小，因为它们更可能是导致Pole倾倒的原因，并且判断越考前的期望价值约大，因为它们\n",
    "#长时间保持了Pole的竖直，和整倾倒的原因没那么大\n",
    "#在CartPole问题中，除了最后结束时刻的Action为0,其余的均为1，。\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    #定义每个Action除直接获得的Reward外的潜在价值running_add\n",
    "    #running_add是从后向前累计的，并且需要经过discount衰减。\n",
    "    #每一个Action的潜在价值，即为后一个Action的前在价值乘以衰减系数gamma，再加上它直接获得的reward\n",
    "    #即running_add * gamma + r[t]\n",
    "    #这样从最后一个Action不断向前累计计算，即可得到全部Action的潜在价值。\n",
    "    for t in reversed(range(r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 25 : 28.120000.\n",
      "Average reward for episode 50 : 21.240000.\n",
      "Average reward for episode 75 : 29.960000.\n",
      "Average reward for episode 100 : 51.600000.\n",
      "Average reward for episode 125 : 53.760000.\n",
      "Average reward for episode 150 : 58.680000.\n",
      "Average reward for episode 175 : 64.600000.\n",
      "Average reward for episode 200 : 77.000000.\n",
      "Average reward for episode 225 : 99.680000.\n",
      "Average reward for episode 250 : 120.600000.\n",
      "Average reward for episode 275 : 157.040000.\n",
      "Average reward for episode 300 : 193.960000.\n",
      "Average reward for episode 325 : 200.000000.\n",
      "Average reward for episode 350 : 200.000000.\n",
      "Average reward for episode 375 : 200.000000.\n",
      "Average reward for episode 400 : 200.000000.\n",
      "Average reward for episode 425 : 200.000000.\n",
      "Average reward for episode 450 : 193.560000.\n",
      "Average reward for episode 475 : 194.640000.\n",
      "Average reward for episode 500 : 194.720000.\n",
      "Average reward for episode 525 : 199.120000.\n",
      "Average reward for episode 550 : 200.000000.\n",
      "Average reward for episode 575 : 200.000000.\n",
      "Average reward for episode 600 : 199.880000.\n",
      "Average reward for episode 625 : 200.000000.\n",
      "Average reward for episode 650 : 198.680000.\n",
      "Average reward for episode 675 : 200.000000.\n",
      "Average reward for episode 700 : 200.000000.\n",
      "Average reward for episode 725 : 200.000000.\n",
      "Average reward for episode 750 : 200.000000.\n",
      "Average reward for episode 775 : 200.000000.\n",
      "Average reward for episode 800 : 200.000000.\n",
      "Average reward for episode 825 : 200.000000.\n",
      "Average reward for episode 850 : 200.000000.\n",
      "Average reward for episode 875 : 200.000000.\n",
      "Average reward for episode 900 : 200.000000.\n",
      "Average reward for episode 925 : 200.000000.\n",
      "Average reward for episode 950 : 200.000000.\n",
      "Average reward for episode 975 : 200.000000.\n",
      "Average reward for episode 1000 : 200.000000.\n",
      "Average reward for episode 1025 : 200.000000.\n",
      "Average reward for episode 1050 : 200.000000.\n",
      "Average reward for episode 1075 : 200.000000.\n",
      "Average reward for episode 1100 : 200.000000.\n",
      "Average reward for episode 1125 : 200.000000.\n",
      "Average reward for episode 1150 : 200.000000.\n",
      "Average reward for episode 1175 : 200.000000.\n",
      "Average reward for episode 1200 : 200.000000.\n",
      "Average reward for episode 1225 : 200.000000.\n",
      "Average reward for episode 1250 : 200.000000.\n",
      "Average reward for episode 1275 : 200.000000.\n",
      "Average reward for episode 1300 : 200.000000.\n",
      "Average reward for episode 1325 : 200.000000.\n",
      "Average reward for episode 1350 : 200.000000.\n",
      "Average reward for episode 1375 : 200.000000.\n"
     ]
    }
   ],
   "source": [
    "#xs为环境信息observation的列表\n",
    "#ys为我们定义的label的列表\n",
    "#drs为我们记录的每一个Action的Reward\n",
    "xs, ys, drs = [], [], []\n",
    "# running_reward = None\n",
    "reward_sum = 0  #累计的Reward\n",
    "episode_number = 1\n",
    "total_episodes = 10000 #总试验次数\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:#创建sess\n",
    "    rendering = False #render的标志关闭，因为会带来较大的延迟\n",
    "    sess.run(init)#初始化全部参数\n",
    "\n",
    "    observation = env.reset()  # Obtain an initial observation of the environment\n",
    "    #先初始化CartPole的环境并获得初始状态\n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in\n",
    "    # gradBuffer until we are ready to update our policy network.\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    #获取所有模型参数,用来创建储存参数梯度的缓冲器gradBuffer\n",
    "\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "        #将gradBuffer全部初始化为0\n",
    "\n",
    "    #接下来，每次试验中，我们将收集参数的梯度存储到gradBuffer中，直到完成了一个batch_size的试验\n",
    "    #再将汇总的梯度更新到模型参数\n",
    "    while episode_number <= total_episodes:\n",
    "\n",
    "        # Rendering the environment slows things down,\n",
    "        # so let's only look at it once our agent is doing a good job.\n",
    "        #当某个batch的平均Reward达到100以上时，即Agent表现良好。\n",
    "        if reward_sum / batch_size > 100 or rendering == True:\n",
    "            #调用env.render()对试验环境进行展示\n",
    "            env.render()\n",
    "            rendering = True\n",
    "\n",
    "        # Make sure the observation is in a shape the network can handle.\n",
    "        #先使用tf.reshape将observation变形为策略网络的输入的格式\n",
    "        x = np.reshape(observation, [1, D])\n",
    "\n",
    "        # Run the policy network and get an action to take.\n",
    "        #然后传入网络中\n",
    "        tfprob = sess.run(probability, feed_dict={observations: x})\n",
    "        #获得网络输出的概率tfprob,即Action取值为1的概率\n",
    "\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        #接下来，我们在0到1之间随机抽样，若随机值小于tfprob，则令Action取值为1,\n",
    "        #否则令Action取值为0,\n",
    "\n",
    "        xs.append(x)  # 将输入的环境信息observation添加到列表xs中\n",
    "        y = 1 if action == 0 else 0  # a \"fake label\" 虚拟的label\n",
    "        ys.append(y)#添加到列表ys中\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #使用env.step执行一次Action，获取observation,reward,done和info\n",
    "\n",
    "        reward_sum += reward\n",
    "        #将reward累加到reward_sum\n",
    "\n",
    "        drs.append(reward)  # record reward (has to be done after we call step() to get reward for previous action)\n",
    "        #并将reward添加到列表drs中\n",
    "\n",
    "        if done:#为True时，表示一次试验结束\n",
    "            episode_number += 1\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            #使用np.vstack将几个列表xs,ys,drs中的元素纵向堆叠起来\n",
    "\n",
    "            xs, ys, drs = [], [], []  # 清空，以备下次使用\n",
    "\n",
    "\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            # 使用前面定义好的discount_rewards函数计算每一步Action的潜在价值\n",
    "            #并进行标准化，得到一个零均值，标准差为1的分布\n",
    "            #discount_reward会参与到模型损失的计算，分布稳定的discount_reward有利于训练的稳定\n",
    "\n",
    "            # Get the gradient for this episode, and save it in the gradBuffer\n",
    "            #将epx,epy,discounted_epr输入网络，并求解梯度\n",
    "            tGrad = sess.run(newGrads, feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix, grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                #将获得的梯度累加到gradBuffer中去\n",
    "\n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0:\n",
    "                #当进行试验的次数达到batch_size的整数倍时，gradBuffer中就积累了足够多的梯度。\n",
    "                #因此使用updateGrads操作将gradBuffer中的梯度更新到策略网络的模型参数中去\n",
    "                sess.run(updateGrads, feed_dict={W1Grad: gradBuffer[0], W2Grad: gradBuffer[1]})\n",
    "                for ix, grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                    #清空gradBuffer为计算下一个batch的梯度做准备\n",
    "\n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                # running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print('Average reward for episode %d : %f.' % (episode_number, reward_sum / batch_size))\n",
    "\n",
    "                if reward_sum / batch_size > 200:\n",
    "                    print(\"Task solved in\", episode_number, 'episodes!')\n",
    "                    break\n",
    "\n",
    "                reward_sum = 0\n",
    "\n",
    "            observation = env.reset()\n",
    "            #每次试验结束后，将任务环境env重置，方便下一次试验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
