{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset() # This gets us the image\n",
    "\n",
    "# hyperparameters\n",
    "episode_number = 0\n",
    "batch_size = 10\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99\n",
    "num_hidden_layer_neurons = 200\n",
    "input_dimensions = 80 * 80\n",
    "learning_rate = 1e-4\n",
    "\n",
    "episode_number = 0\n",
    "reward_sum = 0\n",
    "running_reward = None\n",
    "prev_processed_observations = None\n",
    "\n",
    "weights = {\n",
    "    '1': np.random.randn(num_hidden_layer_neurons, input_dimensions) / np.sqrt(input_dimensions),\n",
    "    '2': np.random.randn(num_hidden_layer_neurons) / np.sqrt(num_hidden_layer_neurons)\n",
    "}\n",
    "\n",
    "# To be used with rmsprop algorithm (http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)\n",
    "expectation_g_squared = {}\n",
    "g_dict = {}\n",
    "for layer_name in weights.keys():\n",
    "    expectation_g_squared[layer_name] = np.zeros_like(weights[layer_name])\n",
    "    g_dict[layer_name] = np.zeros_like(weights[layer_name])\n",
    "\n",
    "episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
    "\n",
    "def downsample(image):\n",
    "    # Take only alternate pixels - basically halves the resolution of the image (which is fine for us)\n",
    "    return image[::2, ::2, :]\n",
    "\n",
    "def remove_color(image):\n",
    "    \"\"\"Convert all color (RGB is the third dimension in the image)\"\"\"\n",
    "    return image[:, :, 0]\n",
    "\n",
    "def remove_background(image):\n",
    "    image[image == 144] = 0\n",
    "    image[image == 109] = 0\n",
    "    return image\n",
    "\n",
    "def preprocess_observations(input_observation, prev_processed_observation, input_dimensions):\n",
    "    \"\"\" convert the 210x160x3 uint8 frame into a 6400 float vector \"\"\"\n",
    "    processed_observation = input_observation[35:195] # crop\n",
    "    processed_observation = downsample(processed_observation)\n",
    "    processed_observation = remove_color(processed_observation)\n",
    "    processed_observation = remove_background(processed_observation)\n",
    "    processed_observation[processed_observation != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    # Convert from 80 x 80 matrix to 1600 x 1 matrix\n",
    "    processed_observation = processed_observation.astype(np.float).ravel()\n",
    "\n",
    "    # subtract the previous frame from the current one so we are only processing on changes in the game\n",
    "    if prev_processed_observation is not None:\n",
    "        input_observation = processed_observation - prev_processed_observation\n",
    "    else:\n",
    "        input_observation = np.zeros(input_dimensions)\n",
    "    # store the previous frame so we can subtract from it next time\n",
    "    prev_processed_observations = processed_observation\n",
    "    return input_observation, prev_processed_observations\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def relu(vector):\n",
    "    vector[vector < 0] = 0\n",
    "    return vector\n",
    "\n",
    "def apply_neural_nets(observation_matrix, weights):\n",
    "    \"\"\" Based on the observation_matrix and weights, compute the new hidden layer values and the new output layer values\"\"\"\n",
    "    hidden_layer_values = np.dot(weights['1'], observation_matrix)\n",
    "    hidden_layer_values = relu(hidden_layer_values)\n",
    "    output_layer_values = np.dot(hidden_layer_values, weights['2'])\n",
    "    output_layer_values = sigmoid(output_layer_values)\n",
    "    return hidden_layer_values, output_layer_values\n",
    "\n",
    "def choose_action(probability):\n",
    "    random_value = np.random.uniform()\n",
    "    if random_value < probability:\n",
    "        # signifies up in openai gym\n",
    "        return 2\n",
    "    else:\n",
    "         # signifies down in openai gym\n",
    "        return 3\n",
    "\n",
    "def compute_gradient(gradient_log_p, hidden_layer_values, observation_values, weights):\n",
    "    \"\"\" See here: http://neuralnetworksanddeeplearning.com/chap2.html\"\"\"\n",
    "    delta_L = gradient_log_p\n",
    "    dC_dw2 = np.dot(hidden_layer_values.T, delta_L).ravel() #Return a contiguous flattened array.\n",
    "    delta_l2 = np.outer(delta_L, weights['2'])#out[i, j] = a[i] * b[j]\n",
    "    delta_l2 = relu(delta_l2)\n",
    "    dC_dw1 = np.dot(delta_l2.T, observation_values)\n",
    "    return {\n",
    "        '1': dC_dw1,\n",
    "        '2': dC_dw2\n",
    "    }\n",
    "\n",
    "def update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate):\n",
    "    \"\"\" See here: http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop\"\"\"\n",
    "    epsilon = 1e-5\n",
    "    for layer_name in weights.keys():\n",
    "        g = g_dict[layer_name]\n",
    "        expectation_g_squared[layer_name] = decay_rate * expectation_g_squared[layer_name] + (1 - decay_rate) * g**2\n",
    "        weights[layer_name] += (learning_rate * g)/(np.sqrt(expectation_g_squared[layer_name] + epsilon))\n",
    "        g_dict[layer_name] = np.zeros_like(weights[layer_name]) # reset batch gradient buffer\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "    \"\"\" Actions you took 20 steps before the end result are less important to the overall result than an action you took a step ago.\n",
    "    This implements that logic by discounting the reward on previous actions based on how long ago they were taken\"\"\"\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, rewards.size)):\n",
    "        if rewards[t] != 0:\n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_with_rewards(gradient_log_p, episode_rewards, gamma):\n",
    "    \"\"\" discount the gradient with the normalized rewards \"\"\"\n",
    "    discounted_episode_rewards = discount_rewards(episode_rewards, gamma)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "    discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
    "    return gradient_log_p * discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-01 23:46:24,435] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(160, 160, 3)\n",
      "(160, 160, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-031fbd716134>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremove_background\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_observation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "print observation.shape\n",
    "processed_observation=observation[35:195]\n",
    "print processed_observation.shape\n",
    "\n",
    "a=remove_background(processed_observation)\n",
    "print a.shape\n",
    "print a.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3)\n",
      "[[[1 2 0]\n",
      "  [1 7 8]]\n",
      "\n",
      " [[0 5 7]\n",
      "  [1 4 5]]\n",
      "\n",
      " [[0 5 7]\n",
      "  [1 4 5]]]\n",
      "[1 2 0 4 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 0],\n",
       "        [1, 7, 8]],\n",
       "\n",
       "       [[0, 5, 7],\n",
       "        [1, 0, 5]],\n",
       "\n",
       "       [[0, 5, 7],\n",
       "        [1, 0, 5]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.array([[[1,2,3],[1,7,8]],[[3,5,7],[1,4,5]],[[3,5,7],[1,4,5]]])\n",
    "print a.shape\n",
    "a[a==3] = 0\n",
    "print a\n",
    "b=np.array([1,2,3,4,5])\n",
    "b[b==3]=0\n",
    "print b\n",
    "def remove_background(image):\n",
    "    image[image == 4] = 0\n",
    "    image[image == 3] = 0\n",
    "    return image\n",
    "remove_background(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -20.000000. running mean: -20.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.010000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.019900\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.029701\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.039404\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.039010\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.038620\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.048234\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.057751\n",
      "1111111111111111111111111\n",
      "{'1': array([[ 0.00987915,  0.01730566, -0.01047021, ...,  0.05431548,\n",
      "         0.11642475, -0.00202735],\n",
      "       [-0.01013475, -0.00040148,  0.01210236, ...,  0.04513826,\n",
      "         0.02527788,  0.006018  ],\n",
      "       [-0.01541359,  0.03231854,  0.00090933, ...,  0.04973625,\n",
      "         0.05513838, -0.0077086 ],\n",
      "       ..., \n",
      "       [ 0.02556545, -0.01513545, -0.01541473, ...,  0.07513488,\n",
      "         0.0716206 , -0.00118286],\n",
      "       [ 0.01317162,  0.00565197,  0.01090977, ...,  0.07985122,\n",
      "         0.07326058,  0.00830596],\n",
      "       [ 0.01149259, -0.03073655, -0.01424688, ...,  0.04382503,\n",
      "         0.1395282 ,  0.00165252]]), '2': array([ 0.19872552, -0.19662298, -0.24928166,  0.21832797,  0.260358  ,\n",
      "        0.23025398,  0.28789381,  0.17416441, -0.27715927, -0.26135051,\n",
      "       -0.30483116, -0.28905096,  0.22395969,  0.27211065, -0.28060686,\n",
      "        0.19752605,  0.22431224, -0.2788633 , -0.29049946,  0.19036088,\n",
      "        0.20961029, -0.26205926,  0.233654  , -0.2756286 , -0.27931725,\n",
      "       -0.23651073,  0.1983585 ,  0.1683271 , -0.35436435, -0.30201588,\n",
      "       -0.24979863, -0.26590323, -0.34067388, -0.28616856, -0.35097964,\n",
      "        0.23212201,  0.20444421, -0.28260023,  0.2607793 ,  0.25729201,\n",
      "        0.30703042, -0.27524681,  0.25422581, -0.32387085,  0.21873864,\n",
      "       -0.33306942, -0.24904122,  0.37697881,  0.21183644,  0.17755024,\n",
      "        0.23822514,  0.3198205 , -0.30700255, -0.23685466, -0.27193329,\n",
      "        0.25121149, -0.24929109,  0.20298193,  0.19340326, -0.39637026,\n",
      "       -0.31163191, -0.2758709 ,  0.2677325 ,  0.24646879,  0.19766746,\n",
      "        0.27764181, -0.28805787, -0.2874735 , -0.35525127,  0.24821926,\n",
      "       -0.25269012, -0.28277913,  0.22028249, -0.2109125 , -0.31024524,\n",
      "       -0.3086071 ,  0.22781577, -0.27177275, -0.27423333,  0.25078669,\n",
      "       -0.36011946,  0.22501189,  0.20628366, -0.43535256, -0.26963054,\n",
      "       -0.36115572,  0.19556232, -0.3012163 , -0.31040285, -0.34283091,\n",
      "       -0.26190811,  0.19246876,  0.25919428, -0.33143906,  0.17862654,\n",
      "        0.17983637,  0.21215431, -0.26202039, -0.29742583, -0.40587604,\n",
      "       -0.26301205,  0.15805279, -0.2959029 ,  0.33873049, -0.27690745,\n",
      "        0.2129778 ,  0.21726956,  0.19559321, -0.32285303, -0.29535212,\n",
      "       -0.29723143, -0.28450596, -0.28141936, -0.40966151, -0.27477386,\n",
      "        0.26042762,  0.20359404, -0.43165588, -0.2582216 , -0.31262839,\n",
      "        0.19306181, -0.35015176, -0.29133476, -0.26860779, -0.25261111,\n",
      "       -0.27503866,  0.21527633, -0.26817885, -0.40043892,  0.17251388,\n",
      "        0.18989926,  0.20970654,  0.20739142, -0.32078729, -0.27194319,\n",
      "       -0.31541541,  0.26886117, -0.30484997, -0.25738827, -0.35589257,\n",
      "        0.19511833,  0.19565745,  0.2248519 ,  0.30135156, -0.31606092,\n",
      "       -0.27629746, -0.28525222, -0.26950794, -0.31726225, -0.28305711,\n",
      "        0.20161112, -0.2042127 , -0.30616587,  0.27593311,  0.29434695,\n",
      "        0.20502155, -0.29520005, -0.30590618, -0.29099868, -0.27193932,\n",
      "        0.19688329, -0.30004557, -0.28859471, -0.33463438, -0.30612673,\n",
      "       -0.28332485,  0.22855157, -0.17426789,  0.25558297,  0.20301629,\n",
      "       -0.27389828,  0.17308806,  0.2598597 , -0.24465612, -0.29006601,\n",
      "        0.22536662, -0.36435871, -0.36097428,  0.2444145 , -0.23607099,\n",
      "       -0.24949875,  0.34965616, -0.31780224, -0.26902249,  0.2031745 ,\n",
      "        0.28680214,  0.19919978, -0.21244587,  0.16204591, -0.35372035,\n",
      "        0.2328852 , -0.27017758, -0.31028106,  0.20327319,  0.20666739,\n",
      "       -0.24860022,  0.18986695, -0.38414207, -0.31498692,  0.28486132])}\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.067174\n",
      "resetting env. episode reward total was -14.000000. running mean: -20.006502\n",
      "resetting env. episode reward total was -15.000000. running mean: -19.956437\n",
      "resetting env. episode reward total was -15.000000. running mean: -19.906873\n",
      "resetting env. episode reward total was -12.000000. running mean: -19.827804\n",
      "resetting env. episode reward total was -8.000000. running mean: -19.709526\n",
      "resetting env. episode reward total was -15.000000. running mean: -19.662431\n",
      "resetting env. episode reward total was -17.000000. running mean: -19.635806\n",
      "resetting env. episode reward total was -12.000000. running mean: -19.559448\n",
      "resetting env. episode reward total was -17.000000. running mean: -19.533854\n",
      "22222222222222222222222222\n",
      "{'1': array([[ 0.00987915,  0.01730566, -0.01047021, ...,  0.05431548,\n",
      "         0.11641141, -0.00202735],\n",
      "       [-0.01013475, -0.00040148,  0.01210236, ...,  0.04506167,\n",
      "         0.0252611 ,  0.006018  ],\n",
      "       [-0.01541359,  0.03231854,  0.00090933, ...,  0.04963932,\n",
      "         0.05507539, -0.0077086 ],\n",
      "       ..., \n",
      "       [ 0.02556545, -0.01513545, -0.01541473, ...,  0.07498648,\n",
      "         0.07160059, -0.00118286],\n",
      "       [ 0.01317162,  0.00565197,  0.01090977, ...,  0.07972909,\n",
      "         0.07324242,  0.00830596],\n",
      "       [ 0.01149259, -0.03073655, -0.01424688, ...,  0.04382503,\n",
      "         0.1395088 ,  0.00165252]]), '2': array([ 0.19786363, -0.19714055, -0.24968331,  0.21791952,  0.25954584,\n",
      "        0.22934625,  0.28739883,  0.17339113, -0.27722613, -0.26199748,\n",
      "       -0.30511841, -0.28973308,  0.22312194,  0.27155714, -0.28134027,\n",
      "        0.19674569,  0.22340404, -0.27965482, -0.29126657,  0.18944299,\n",
      "        0.20904712, -0.26264673,  0.23294764, -0.27562193, -0.28020325,\n",
      "       -0.23665596,  0.1975645 ,  0.16749079, -0.3550405 , -0.30221164,\n",
      "       -0.25070285, -0.26628867, -0.34087163, -0.28646776, -0.35133735,\n",
      "        0.23129041,  0.20381247, -0.28282343,  0.26029601,  0.25657799,\n",
      "        0.30614458, -0.27565946,  0.25341354, -0.32471889,  0.21797408,\n",
      "       -0.33392704, -0.24953087,  0.37655046,  0.21107763,  0.17686492,\n",
      "        0.23736341,  0.31912541, -0.30799454, -0.23719319, -0.27219085,\n",
      "        0.25048756, -0.25015465,  0.20220799,  0.19263513, -0.39705906,\n",
      "       -0.31217873, -0.2768092 ,  0.2668406 ,  0.24592675,  0.19692896,\n",
      "        0.27681162, -0.28897827, -0.28825311, -0.35557074,  0.24777882,\n",
      "       -0.25326343, -0.28337817,  0.2195025 , -0.21145692, -0.31087997,\n",
      "       -0.30908534,  0.22726335, -0.27248193, -0.27453426,  0.25010905,\n",
      "       -0.36080492,  0.22423807,  0.2059728 , -0.43563024, -0.26946278,\n",
      "       -0.36111901,  0.19488761, -0.30133459, -0.3112001 , -0.34303769,\n",
      "       -0.26268279,  0.19188334,  0.25859665, -0.33183408,  0.17794341,\n",
      "        0.1791633 ,  0.21139758, -0.26227272, -0.29782254, -0.40645616,\n",
      "       -0.263484  ,  0.15735903, -0.29605126,  0.33780383, -0.27731703,\n",
      "        0.21217757,  0.21634137,  0.19495773, -0.32359427, -0.29611442,\n",
      "       -0.29785513, -0.28522694, -0.28186266, -0.41045286, -0.27530324,\n",
      "        0.25949301,  0.20284483, -0.43211076, -0.25860684, -0.31255842,\n",
      "        0.19242876, -0.35035134, -0.29215427, -0.26924757, -0.25322106,\n",
      "       -0.27496343,  0.2146054 , -0.26833524, -0.40054155,  0.17171405,\n",
      "        0.1890081 ,  0.20880058,  0.20655695, -0.3211352 , -0.27272303,\n",
      "       -0.31593111,  0.26797515, -0.30564213, -0.25789726, -0.35609453,\n",
      "        0.1943257 ,  0.19488513,  0.22427432,  0.30068083, -0.31626562,\n",
      "       -0.27662715, -0.28575943, -0.27007664, -0.31808756, -0.28376727,\n",
      "        0.20068824, -0.20444383, -0.30683184,  0.27505631,  0.29347817,\n",
      "        0.20429635, -0.29606955, -0.30648135, -0.29152329, -0.27281934,\n",
      "        0.19593711, -0.30093241, -0.28949781, -0.33518301, -0.30692595,\n",
      "       -0.28413198,  0.22756571, -0.17524569,  0.25459468,  0.20203773,\n",
      "       -0.27456553,  0.17209231,  0.25916265, -0.24473107, -0.29038142,\n",
      "        0.22441429, -0.36457406, -0.36091848,  0.24381862, -0.23702036,\n",
      "       -0.24999462,  0.34882505, -0.318545  , -0.26943757,  0.2024333 ,\n",
      "        0.28594029,  0.19863176, -0.21302069,  0.16111913, -0.35403063,\n",
      "        0.23202963, -0.26980343, -0.31073948,  0.20273531,  0.20577603,\n",
      "       -0.24883082,  0.18917312, -0.38493481, -0.31594027,  0.28414982])}\n",
      "resetting env. episode reward total was -10.000000. running mean: -19.438515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -9.000000. running mean: -19.334130\n",
      "resetting env. episode reward total was -15.000000. running mean: -19.290789\n",
      "resetting env. episode reward total was -16.000000. running mean: -19.257881\n",
      "resetting env. episode reward total was -14.000000. running mean: -19.205302\n",
      "resetting env. episode reward total was -19.000000. running mean: -19.203249\n",
      "resetting env. episode reward total was -18.000000. running mean: -19.191217\n",
      "resetting env. episode reward total was -14.000000. running mean: -19.139304\n",
      "resetting env. episode reward total was -21.000000. running mean: -19.157911\n",
      "resetting env. episode reward total was -12.000000. running mean: -19.086332\n",
      "22222222222222222222222222\n",
      "{'1': array([[ 0.00987915,  0.01730566, -0.01047021, ...,  0.05431548,\n",
      "         0.11690057, -0.00202735],\n",
      "       [-0.01013475, -0.00040148,  0.01210236, ...,  0.04506081,\n",
      "         0.02478506,  0.006018  ],\n",
      "       [-0.01541359,  0.03231854,  0.00090933, ...,  0.04963823,\n",
      "         0.05417872, -0.0077086 ],\n",
      "       ..., \n",
      "       [ 0.02556545, -0.01513545, -0.01541473, ...,  0.0749848 ,\n",
      "         0.07105836, -0.00118286],\n",
      "       [ 0.01317162,  0.00565197,  0.01090977, ...,  0.07972772,\n",
      "         0.07273681,  0.00830596],\n",
      "       [ 0.01149259, -0.03073655, -0.01424688, ...,  0.04382503,\n",
      "         0.14014147,  0.00165252]]), '2': array([ 0.19796665, -0.19759616, -0.25006914,  0.21749291,  0.2591887 ,\n",
      "        0.2293593 ,  0.28712124,  0.17303738, -0.27742761, -0.26231014,\n",
      "       -0.30504045, -0.29032216,  0.22332045,  0.27122088, -0.28207019,\n",
      "        0.1966571 ,  0.22340376, -0.27964383, -0.29204933,  0.18953483,\n",
      "        0.20867275, -0.26320733,  0.23287305, -0.27603445, -0.28085181,\n",
      "       -0.23661396,  0.1971769 ,  0.16746169, -0.35513497, -0.30245203,\n",
      "       -0.25134956, -0.26700827, -0.34070607, -0.28688304, -0.35151026,\n",
      "        0.23146548,  0.20369433, -0.28268789,  0.26028648,  0.25653736,\n",
      "        0.30604375, -0.27526424,  0.25302037, -0.32558868,  0.21779303,\n",
      "       -0.33429205, -0.25019321,  0.37648392,  0.21130628,  0.1766184 ,\n",
      "        0.23738655,  0.31877909, -0.30866338, -0.23761928, -0.27246064,\n",
      "        0.25066963, -0.25057754,  0.20192186,  0.19256098, -0.39730127,\n",
      "       -0.3121465 , -0.2768964 ,  0.26639763,  0.2457876 ,  0.196379  ,\n",
      "        0.27651678, -0.28921555, -0.28811295, -0.35593751,  0.24758634,\n",
      "       -0.25345443, -0.28406929,  0.21936461, -0.21176544, -0.31118899,\n",
      "       -0.30965367,  0.22719718, -0.27217235, -0.27456087,  0.25016611,\n",
      "       -0.36124597,  0.22380163,  0.20617109, -0.43559743, -0.2696195 ,\n",
      "       -0.36121263,  0.1947251 , -0.30173723, -0.31179468, -0.34293336,\n",
      "       -0.26297087,  0.19190005,  0.25819062, -0.33164304,  0.1779933 ,\n",
      "        0.17940018,  0.2109664 , -0.2626336 , -0.29790764, -0.40699448,\n",
      "       -0.26404039,  0.15744794, -0.29639378,  0.33777611, -0.2779136 ,\n",
      "        0.21202359,  0.21610902,  0.19511071, -0.32380283, -0.29669904,\n",
      "       -0.29825795, -0.28538261, -0.28202328, -0.41091487, -0.27565054,\n",
      "        0.25928255,  0.20267974, -0.43213206, -0.25936782, -0.31328757,\n",
      "        0.19231412, -0.35079977, -0.2923387 , -0.26985041, -0.25384359,\n",
      "       -0.27548927,  0.21453403, -0.26878855, -0.40079932,  0.17150913,\n",
      "        0.18859987,  0.20842653,  0.20629223, -0.32196106, -0.27310529,\n",
      "       -0.31613124,  0.26770088, -0.30560988, -0.25862573, -0.35590606,\n",
      "        0.19425909,  0.19475889,  0.22422256,  0.3002341 , -0.31612851,\n",
      "       -0.27687059, -0.28645382, -0.27031625, -0.31816857, -0.28394085,\n",
      "        0.20066336, -0.2047995 , -0.30694795,  0.27466795,  0.29361554,\n",
      "        0.20408823, -0.29624188, -0.3064626 , -0.29218805, -0.27322782,\n",
      "        0.19579122, -0.30135607, -0.28937756, -0.33469886, -0.3075761 ,\n",
      "       -0.28432336,  0.22725748, -0.17574112,  0.25443952,  0.20162086,\n",
      "       -0.27525398,  0.17223517,  0.25909907, -0.24524935, -0.29063505,\n",
      "        0.22422101, -0.36471192, -0.36109422,  0.24365575, -0.23774732,\n",
      "       -0.25001292,  0.34834967, -0.3182263 , -0.26981933,  0.20209737,\n",
      "        0.28573116,  0.19845454, -0.21347858,  0.16168078, -0.35430503,\n",
      "        0.23178725, -0.27065742, -0.31099058,  0.20179161,  0.20579643,\n",
      "       -0.24935437,  0.18900934, -0.38510214, -0.31592004,  0.28383932])}\n",
      "resetting env. episode reward total was -16.000000. running mean: -19.055469\n",
      "resetting env. episode reward total was -15.000000. running mean: -19.014914\n",
      "resetting env. episode reward total was -13.000000. running mean: -18.954765\n",
      "resetting env. episode reward total was -13.000000. running mean: -18.895217\n",
      "resetting env. episode reward total was -19.000000. running mean: -18.896265\n",
      "resetting env. episode reward total was -17.000000. running mean: -18.877303\n",
      "resetting env. episode reward total was -17.000000. running mean: -18.858530\n",
      "resetting env. episode reward total was -17.000000. running mean: -18.839944\n",
      "resetting env. episode reward total was -11.000000. running mean: -18.761545\n",
      "resetting env. episode reward total was -12.000000. running mean: -18.693929\n",
      "22222222222222222222222222\n",
      "{'1': array([[ 0.00987915,  0.01730566, -0.01047021, ...,  0.05431548,\n",
      "         0.11676133, -0.00202735],\n",
      "       [-0.01013475, -0.00040148,  0.01210236, ...,  0.0449998 ,\n",
      "         0.02446356,  0.006018  ],\n",
      "       [-0.01541359,  0.03231854,  0.00090933, ...,  0.04956125,\n",
      "         0.05364033, -0.0077086 ],\n",
      "       ..., \n",
      "       [ 0.02556545, -0.01513545, -0.01541473, ...,  0.07486746,\n",
      "         0.0706982 , -0.00118286],\n",
      "       [ 0.01317162,  0.00565197,  0.01090977, ...,  0.0796309 ,\n",
      "         0.07239825,  0.00830596],\n",
      "       [ 0.01149259, -0.03073655, -0.01424688, ...,  0.04382502,\n",
      "         0.13996283,  0.00165252]]), '2': array([ 0.19742113, -0.19760757, -0.25022231,  0.2171974 ,  0.25887725,\n",
      "        0.22907423,  0.28677244,  0.17241881, -0.27727326, -0.26270526,\n",
      "       -0.30559562, -0.29070654,  0.22278409,  0.27092639, -0.28209857,\n",
      "        0.19643349,  0.22271399, -0.27987724, -0.29263603,  0.18884892,\n",
      "        0.20833949, -0.26362574,  0.23246364, -0.27631698, -0.28087159,\n",
      "       -0.23676699,  0.19665322,  0.16731863, -0.35514815, -0.30271247,\n",
      "       -0.25163986, -0.26754505, -0.34132321, -0.28701506, -0.35168356,\n",
      "        0.23119898,  0.20343808, -0.28321109,  0.26002182,  0.25584168,\n",
      "        0.30561078, -0.27596326,  0.25264583, -0.32610571,  0.2171228 ,\n",
      "       -0.33496746, -0.25027226,  0.37594275,  0.21096965,  0.17590721,\n",
      "        0.23686005,  0.31851094, -0.30871536, -0.23801847, -0.27285496,\n",
      "        0.25013614, -0.2509234 ,  0.20149936,  0.19245781, -0.39746522,\n",
      "       -0.31240269, -0.27716653,  0.26611812,  0.24503462,  0.19574218,\n",
      "        0.2763286 , -0.28955088, -0.28870823, -0.35622956,  0.24713712,\n",
      "       -0.25365988, -0.28452388,  0.21903597, -0.21211558, -0.31167823,\n",
      "       -0.30928262,  0.22677924, -0.27272723, -0.27515374,  0.249869  ,\n",
      "       -0.36134846,  0.22314784,  0.20537557, -0.43572462, -0.26996975,\n",
      "       -0.36188189,  0.19422646, -0.30239831, -0.31202228, -0.34314988,\n",
      "       -0.26350972,  0.19155101,  0.25771193, -0.33178374,  0.17770566,\n",
      "        0.17912246,  0.21043572, -0.26301522, -0.29820817, -0.40685362,\n",
      "       -0.26467126,  0.15724922, -0.29676733,  0.33735752, -0.27818881,\n",
      "        0.21187734,  0.2160457 ,  0.19479483, -0.32387289, -0.29647063,\n",
      "       -0.29863701, -0.28561938, -0.28279044, -0.41070685, -0.2760923 ,\n",
      "        0.25875054,  0.2021766 , -0.43213731, -0.25926187, -0.31365666,\n",
      "        0.19178166, -0.35153143, -0.29259149, -0.27019549, -0.25443754,\n",
      "       -0.27598514,  0.21413984, -0.2691833 , -0.4008996 ,  0.17096241,\n",
      "        0.18813611,  0.20792854,  0.20584246, -0.32273307, -0.27319094,\n",
      "       -0.31588596,  0.2675225 , -0.3057228 , -0.25898106, -0.3562649 ,\n",
      "        0.19361198,  0.19440531,  0.22369999,  0.3000766 , -0.31669151,\n",
      "       -0.27697745, -0.2870009 , -0.27040677, -0.31830104, -0.28419125,\n",
      "        0.20000692, -0.2051542 , -0.30730117,  0.2741948 ,  0.29343432,\n",
      "        0.20377482, -0.2961953 , -0.30665969, -0.29213372, -0.27362565,\n",
      "        0.19555105, -0.301677  , -0.28944288, -0.33496344, -0.30760037,\n",
      "       -0.28470167,  0.22654397, -0.17607542,  0.25405101,  0.20101147,\n",
      "       -0.2756254 ,  0.17178894,  0.25860702, -0.24588986, -0.29152221,\n",
      "        0.22405295, -0.36508754, -0.3614371 ,  0.24330108, -0.23776024,\n",
      "       -0.25006468,  0.34779924, -0.31813112, -0.2700186 ,  0.20189994,\n",
      "        0.2853218 ,  0.19790282, -0.21356318,  0.16103875, -0.3546504 ,\n",
      "        0.23121419, -0.27133714, -0.31119013,  0.20088189,  0.20536201,\n",
      "       -0.24956753,  0.18882917, -0.38521621, -0.31590594,  0.28331282])}\n",
      "resetting env. episode reward total was -16.000000. running mean: -18.666990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -11.000000. running mean: -18.590320\n",
      "resetting env. episode reward total was -14.000000. running mean: -18.544417\n",
      "resetting env. episode reward total was -14.000000. running mean: -18.498973\n",
      "resetting env. episode reward total was -14.000000. running mean: -18.453983\n",
      "resetting env. episode reward total was -18.000000. running mean: -18.449443\n",
      "resetting env. episode reward total was -19.000000. running mean: -18.454949\n",
      "resetting env. episode reward total was -18.000000. running mean: -18.450399\n",
      "resetting env. episode reward total was -11.000000. running mean: -18.375895\n",
      "resetting env. episode reward total was -16.000000. running mean: -18.352136\n",
      "22222222222222222222222222\n",
      "{'1': array([[ 0.00987915,  0.01730566, -0.01047021, ...,  0.05431548,\n",
      "         0.11673605, -0.00202735],\n",
      "       [-0.01013475, -0.00040148,  0.01210236, ...,  0.0450006 ,\n",
      "         0.02453359,  0.006018  ],\n",
      "       [-0.01541359,  0.03231854,  0.00090933, ...,  0.04956226,\n",
      "         0.05375714, -0.0077086 ],\n",
      "       ..., \n",
      "       [ 0.02556545, -0.01513545, -0.01541473, ...,  0.074869  ,\n",
      "         0.07077663, -0.00118286],\n",
      "       [ 0.01317162,  0.00565197,  0.01090977, ...,  0.07963217,\n",
      "         0.07247198,  0.00830596],\n",
      "       [ 0.01149259, -0.03073655, -0.01424688, ...,  0.04382502,\n",
      "         0.13993037,  0.00165252]]), '2': array([ 0.19760749, -0.19739226, -0.25010425,  0.21744315,  0.25925285,\n",
      "        0.22933243,  0.28726595,  0.17281553, -0.27729405, -0.26228626,\n",
      "       -0.3056541 , -0.29030608,  0.22318969,  0.2714174 , -0.2821466 ,\n",
      "        0.19677512,  0.22315911, -0.27985177, -0.2928253 ,  0.18910906,\n",
      "        0.2088482 , -0.26323615,  0.23274642, -0.27642599, -0.28059246,\n",
      "       -0.23723535,  0.19715162,  0.16785103, -0.35549153, -0.30289419,\n",
      "       -0.25172852, -0.26722718, -0.34089088, -0.28701333, -0.35149295,\n",
      "        0.23124592,  0.20396646, -0.28336841,  0.26032189,  0.25630329,\n",
      "        0.3060356 , -0.27610371,  0.2529391 , -0.32646992,  0.21747797,\n",
      "       -0.33432945, -0.2500609 ,  0.37617386,  0.21115682,  0.17641689,\n",
      "        0.23722848,  0.31897315, -0.30849671, -0.23787172, -0.27273643,\n",
      "        0.25047695, -0.25085895,  0.2020373 ,  0.19268915, -0.39735326,\n",
      "       -0.3125041 , -0.27716752,  0.26645258,  0.24540893,  0.19627293,\n",
      "        0.27685377, -0.28952041, -0.28864798, -0.35602308,  0.24752561,\n",
      "       -0.25353043, -0.2843339 ,  0.2195193 , -0.21179597, -0.31146135,\n",
      "       -0.30937172,  0.22710285, -0.27262537, -0.27515996,  0.25005673,\n",
      "       -0.36109238,  0.22348156,  0.20582495, -0.43566348, -0.27005757,\n",
      "       -0.36185284,  0.19479565, -0.30246568, -0.31212348, -0.34329966,\n",
      "       -0.26311346,  0.19205288,  0.25827193, -0.33177836,  0.17804352,\n",
      "        0.17939025,  0.21087546, -0.26328121, -0.29851271, -0.40646911,\n",
      "       -0.26473963,  0.15784128, -0.29692975,  0.33778033, -0.27827431,\n",
      "        0.21216567,  0.21655811,  0.19494505, -0.32372545, -0.29666261,\n",
      "       -0.29843218, -0.28579732, -0.28319975, -0.41107353, -0.27601636,\n",
      "        0.25918708,  0.20252475, -0.43241971, -0.25872112, -0.3134221 ,\n",
      "        0.19201156, -0.35149741, -0.29228267, -0.26995602, -0.25410516,\n",
      "       -0.27600927,  0.21454482, -0.26913028, -0.40091553,  0.17157   ,\n",
      "        0.18869084,  0.20839729,  0.20647321, -0.32259448, -0.27310753,\n",
      "       -0.31602164,  0.26804035, -0.3054234 , -0.25868162, -0.35606706,\n",
      "        0.19417363,  0.19507053,  0.224005  ,  0.3002894 , -0.31673511,\n",
      "       -0.27695205, -0.28703195, -0.27013268, -0.31827104, -0.28370103,\n",
      "        0.20054872, -0.20485231, -0.30699066,  0.27460736,  0.29380677,\n",
      "        0.20390353, -0.29606409, -0.30676976, -0.29179232, -0.27340582,\n",
      "        0.19586742, -0.30204761, -0.28958703, -0.33527338, -0.30752385,\n",
      "       -0.2842662 ,  0.22701494, -0.17556831,  0.25455715,  0.20162304,\n",
      "       -0.27505413,  0.1720341 ,  0.25876878, -0.24575831, -0.29127755,\n",
      "        0.22440534, -0.36503072, -0.36147938,  0.24364252, -0.23766226,\n",
      "       -0.25024604,  0.34824579, -0.3180577 , -0.27043833,  0.20246524,\n",
      "        0.28566953,  0.19840259, -0.2132451 ,  0.16176393, -0.35470157,\n",
      "        0.23175068, -0.27093759, -0.31112138,  0.20145901,  0.20555152,\n",
      "       -0.24931305,  0.18923855, -0.38495086, -0.31573193,  0.28376702])}\n",
      "resetting env. episode reward total was -15.000000. running mean: -18.318615\n",
      "resetting env. episode reward total was -9.000000. running mean: -18.225429\n",
      "resetting env. episode reward total was -13.000000. running mean: -18.173175\n",
      "resetting env. episode reward total was -13.000000. running mean: -18.121443\n",
      "resetting env. episode reward total was -19.000000. running mean: -18.130228\n",
      "resetting env. episode reward total was -15.000000. running mean: -18.098926\n",
      "resetting env. episode reward total was -19.000000. running mean: -18.107937\n",
      "resetting env. episode reward total was -13.000000. running mean: -18.056858\n",
      "resetting env. episode reward total was -6.000000. running mean: -17.936289\n",
      "resetting env. episode reward total was -14.000000. running mean: -17.896926\n",
      "22222222222222222222222222\n",
      "{'1': array([[ 0.00987915,  0.01730566, -0.0104785 , ...,  0.05431548,\n",
      "         0.11670415, -0.00202735],\n",
      "       [-0.01013475, -0.00040148,  0.01210269, ...,  0.04500056,\n",
      "         0.02434527,  0.006018  ],\n",
      "       [-0.01541359,  0.03231854,  0.00090975, ...,  0.0495622 ,\n",
      "         0.05345239, -0.0077086 ],\n",
      "       ..., \n",
      "       [ 0.02556545, -0.01513545, -0.01541409, ...,  0.07486891,\n",
      "         0.07056656, -0.00118286],\n",
      "       [ 0.01317162,  0.00565197,  0.01091029, ...,  0.07963209,\n",
      "         0.07227398,  0.00830596],\n",
      "       [ 0.01149259, -0.03073655, -0.01425878, ...,  0.04382501,\n",
      "         0.13988939,  0.00165252]]), '2': array([ 0.19780903, -0.19706163, -0.24996761,  0.2174931 ,  0.25921695,\n",
      "        0.2294206 ,  0.28732021,  0.17286169, -0.27684965, -0.26220956,\n",
      "       -0.30548983, -0.28981849,  0.22297037,  0.27159868, -0.28177118,\n",
      "        0.1967339 ,  0.2233248 , -0.28004128, -0.29311071,  0.18888533,\n",
      "        0.20883145, -0.2634651 ,  0.23270255, -0.27629452, -0.28030769,\n",
      "       -0.23725858,  0.19745807,  0.16786932, -0.35564279, -0.30263028,\n",
      "       -0.25164082, -0.26717933, -0.34111148, -0.28692474, -0.35174391,\n",
      "        0.23149458,  0.20388025, -0.28320436,  0.26042902,  0.256447  ,\n",
      "        0.30607785, -0.27658094,  0.25284447, -0.32677093,  0.21768278,\n",
      "       -0.33404734, -0.25005914,  0.37623727,  0.21132513,  0.17623576,\n",
      "        0.23736263,  0.31896003, -0.30797271, -0.23782083, -0.27315454,\n",
      "        0.25058465, -0.25101064,  0.2019811 ,  0.19281679, -0.39766427,\n",
      "       -0.3127201 , -0.27733889,  0.26685025,  0.24519601,  0.19633811,\n",
      "        0.27679683, -0.28970258, -0.28873234, -0.35604268,  0.2476425 ,\n",
      "       -0.25351085, -0.28446426,  0.21976019, -0.21166995, -0.31152544,\n",
      "       -0.30912358,  0.22740195, -0.27290133, -0.27496705,  0.2503137 ,\n",
      "       -0.36121357,  0.22331363,  0.20567434, -0.43540476, -0.27022878,\n",
      "       -0.36211246,  0.19501396, -0.30193184, -0.31260117, -0.34299297,\n",
      "       -0.26335154,  0.19181897,  0.25831196, -0.33168856,  0.17810705,\n",
      "        0.17969443,  0.21108675, -0.26332619, -0.29852386, -0.40630896,\n",
      "       -0.26442229,  0.15753924, -0.2967819 ,  0.33787662, -0.27898724,\n",
      "        0.21244977,  0.21678599,  0.19489964, -0.32370105, -0.2964254 ,\n",
      "       -0.29859349, -0.28565846, -0.28357351, -0.41131522, -0.2762218 ,\n",
      "        0.2595246 ,  0.20262271, -0.43217487, -0.25853655, -0.31358312,\n",
      "        0.19223734, -0.35183127, -0.29212515, -0.27021694, -0.25388889,\n",
      "       -0.27600307,  0.21472736, -0.26887289, -0.40044247,  0.17182535,\n",
      "        0.18831609,  0.20826395,  0.20636229, -0.32188966, -0.27290033,\n",
      "       -0.31611853,  0.26797161, -0.30538855, -0.25874583, -0.35570709,\n",
      "        0.19432934,  0.19525491,  0.22409903,  0.30040807, -0.31650612,\n",
      "       -0.27744885, -0.28682825, -0.26992962, -0.31830483, -0.28385947,\n",
      "        0.20081754, -0.20477785, -0.30707026,  0.27428115,  0.29394344,\n",
      "        0.20364541, -0.29619351, -0.30658346, -0.29165159, -0.27319566,\n",
      "        0.19605526, -0.30212032, -0.28938153, -0.33524205, -0.30743828,\n",
      "       -0.28404684,  0.22701776, -0.17582336,  0.25450268,  0.20172622,\n",
      "       -0.27512268,  0.1725696 ,  0.25860157, -0.2457052 , -0.29095712,\n",
      "        0.22416904, -0.36503026, -0.36119139,  0.24351679, -0.23738812,\n",
      "       -0.25011324,  0.34843022, -0.31852451, -0.27054011,  0.20244854,\n",
      "        0.28592471,  0.19849908, -0.21329075,  0.16151854, -0.35500434,\n",
      "        0.23181144, -0.27085745, -0.31112794,  0.20144545,  0.20524685,\n",
      "       -0.24927081,  0.18891828, -0.38483701, -0.31598815,  0.28388697])}\n",
      "resetting env. episode reward total was -17.000000. running mean: -17.887957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was 2.000000. running mean: -17.689077\n",
      "resetting env. episode reward total was -12.000000. running mean: -17.632186\n",
      "resetting env. episode reward total was -17.000000. running mean: -17.625865\n",
      "resetting env. episode reward total was -14.000000. running mean: -17.589606\n",
      "resetting env. episode reward total was -13.000000. running mean: -17.543710\n",
      "resetting env. episode reward total was -13.000000. running mean: -17.498273\n",
      "resetting env. episode reward total was -14.000000. running mean: -17.463290\n",
      "resetting env. episode reward total was -14.000000. running mean: -17.428657\n",
      "resetting env. episode reward total was -19.000000. running mean: -17.444371\n",
      "22222222222222222222222222\n",
      "{'1': array([[ 0.00987915,  0.01730566, -0.0104785 , ...,  0.05428426,\n",
      "         0.11670245, -0.00202735],\n",
      "       [-0.01013475, -0.00040148,  0.01210269, ...,  0.04499949,\n",
      "         0.02392169,  0.006018  ],\n",
      "       [-0.01541359,  0.03231854,  0.00090975, ...,  0.04956085,\n",
      "         0.05284858, -0.0077086 ],\n",
      "       ..., \n",
      "       [ 0.02556545, -0.01513545, -0.01541409, ...,  0.07486685,\n",
      "         0.07010354, -0.00118286],\n",
      "       [ 0.01317162,  0.00565197,  0.01091029, ...,  0.07963039,\n",
      "         0.07183185,  0.00830596],\n",
      "       [ 0.01149259, -0.03073655, -0.01425878, ...,  0.04378024,\n",
      "         0.13988721,  0.00165252]]), '2': array([ 0.1982322 , -0.19706597, -0.2500115 ,  0.21778411,  0.25957862,\n",
      "        0.23006872,  0.28775093,  0.17328337, -0.27708873, -0.26219279,\n",
      "       -0.30525937, -0.28948062,  0.22362474,  0.27188888, -0.28182014,\n",
      "        0.19723731,  0.22393448, -0.27961939, -0.29281406,  0.18927559,\n",
      "        0.2090793 , -0.2636234 ,  0.23303025, -0.2760517 , -0.28057158,\n",
      "       -0.23796479,  0.19792696,  0.16820974, -0.35581361, -0.30226736,\n",
      "       -0.25177676, -0.26691267, -0.34054793, -0.28670691, -0.35184498,\n",
      "        0.23214539,  0.204199  , -0.28292724,  0.26076746,  0.25706178,\n",
      "        0.30653777, -0.27618239,  0.25339266, -0.32679684,  0.21829826,\n",
      "       -0.33385584, -0.24998084,  0.376745  ,  0.21167349,  0.17662807,\n",
      "        0.23764211,  0.31921818, -0.30778646, -0.23831963, -0.2732328 ,\n",
      "        0.25104039, -0.25105881,  0.20260654,  0.19322361, -0.39739313,\n",
      "       -0.31286077, -0.27750272,  0.26749092,  0.24570516,  0.19656278,\n",
      "        0.27703611, -0.28963861, -0.28832976, -0.35612462,  0.24802014,\n",
      "       -0.25352827, -0.28479802,  0.22028192, -0.21196825, -0.31139312,\n",
      "       -0.30915922,  0.22795207, -0.27279011, -0.27509512,  0.25087276,\n",
      "       -0.3610881 ,  0.22378133,  0.20604306, -0.43598146, -0.2700318 ,\n",
      "       -0.36230613,  0.19556735, -0.30168366, -0.31281221, -0.34295581,\n",
      "       -0.26315998,  0.192295  ,  0.25879925, -0.33154993,  0.1786774 ,\n",
      "        0.18022034,  0.21156117, -0.26312424, -0.29841012, -0.40661423,\n",
      "       -0.26433575,  0.15795421, -0.29656213,  0.33830987, -0.27854535,\n",
      "        0.2130331 ,  0.21738721,  0.19531676, -0.32367736, -0.29645316,\n",
      "       -0.29862987, -0.28572744, -0.28360488, -0.41077077, -0.2766235 ,\n",
      "        0.26003056,  0.20291944, -0.43193126, -0.25862829, -0.31384481,\n",
      "        0.1926159 , -0.35183168, -0.29184326, -0.27016157, -0.25424381,\n",
      "       -0.27603992,  0.21510417, -0.26873152, -0.40024291,  0.17239406,\n",
      "        0.18843202,  0.20876759,  0.20692382, -0.32176633, -0.2728112 ,\n",
      "       -0.31612848,  0.26848386, -0.30497385, -0.25900133, -0.35531512,\n",
      "        0.19474271,  0.19575375,  0.22457076,  0.30084785, -0.31601405,\n",
      "       -0.2776951 , -0.28697871, -0.26988687, -0.31805557, -0.28370112,\n",
      "        0.20142392, -0.20528503, -0.30681317,  0.27456403,  0.29457941,\n",
      "        0.20401322, -0.2958199 , -0.30637513, -0.292063  , -0.27302575,\n",
      "        0.19644571, -0.30204264, -0.28943557, -0.33497001, -0.30769549,\n",
      "       -0.28413087,  0.22744523, -0.17606281,  0.25483485,  0.20226646,\n",
      "       -0.27527171,  0.17297154,  0.25904649, -0.24587195, -0.29118538,\n",
      "        0.22457016, -0.36511135, -0.36126914,  0.24393817, -0.23798912,\n",
      "       -0.24977099,  0.34885215, -0.31809049, -0.27069491,  0.203022  ,\n",
      "        0.28640662,  0.19898784, -0.21308873,  0.16185034, -0.35451567,\n",
      "        0.23225247, -0.27104597, -0.31093358,  0.20177024,  0.20564107,\n",
      "       -0.24938265,  0.18936299, -0.38509437, -0.31536975,  0.28420269])}\n",
      "resetting env. episode reward total was -10.000000. running mean: -17.369927\n",
      "resetting env. episode reward total was -5.000000. running mean: -17.246228\n",
      "resetting env. episode reward total was -15.000000. running mean: -17.223765\n",
      "resetting env. episode reward total was -12.000000. running mean: -17.171528\n",
      "resetting env. episode reward total was -10.000000. running mean: -17.099812\n",
      "resetting env. episode reward total was -17.000000. running mean: -17.098814\n",
      "resetting env. episode reward total was -9.000000. running mean: -17.017826\n",
      "resetting env. episode reward total was -12.000000. running mean: -16.967648\n",
      "resetting env. episode reward total was -14.000000. running mean: -16.937971\n",
      "resetting env. episode reward total was -12.000000. running mean: -16.888592\n",
      "22222222222222222222222222\n",
      "{'1': array([[ 0.00987915,  0.01730566, -0.0104785 , ...,  0.05428426,\n",
      "         0.1165266 , -0.00202735],\n",
      "       [-0.01013475, -0.00040148,  0.01210269, ...,  0.04499942,\n",
      "         0.02394658,  0.006018  ],\n",
      "       [-0.01541359,  0.03231854,  0.00090975, ...,  0.04956076,\n",
      "         0.05288405, -0.0077086 ],\n",
      "       ..., \n",
      "       [ 0.02556545, -0.01513545, -0.01541409, ...,  0.07486671,\n",
      "         0.07013076, -0.00118286],\n",
      "       [ 0.01317162,  0.00565197,  0.01091029, ...,  0.07963028,\n",
      "         0.07185777,  0.00830596],\n",
      "       [ 0.01149259, -0.03073655, -0.01425878, ...,  0.04378024,\n",
      "         0.13966394,  0.00165252]]), '2': array([ 0.19895495, -0.19637576, -0.24934221,  0.21853093,  0.26027317,\n",
      "        0.23080139,  0.28851819,  0.17406363, -0.27633236, -0.26182641,\n",
      "       -0.30494317, -0.28870915,  0.22428639,  0.2726878 , -0.28123921,\n",
      "        0.19791994,  0.22457146, -0.27903302, -0.29226623,  0.1900487 ,\n",
      "        0.20979063, -0.2631811 ,  0.23375486, -0.27527878, -0.27996926,\n",
      "       -0.23713028,  0.19868981,  0.16888073, -0.35522843, -0.30174098,\n",
      "       -0.25112025, -0.26620752, -0.34013703, -0.28615647, -0.3516168 ,\n",
      "        0.23276433,  0.20491918, -0.28238167,  0.26128026,  0.25787229,\n",
      "        0.30723645, -0.27552964,  0.25411201, -0.32631327,  0.21900367,\n",
      "       -0.33320225, -0.24924987,  0.3774762 ,  0.21246814,  0.17739521,\n",
      "        0.23837084,  0.3198756 , -0.3070368 , -0.23783767, -0.27287571,\n",
      "        0.25175294, -0.25050982,  0.20327841,  0.19393177, -0.3969688 ,\n",
      "       -0.31228694, -0.27669541,  0.26817568,  0.24645711,  0.19727623,\n",
      "        0.27775751, -0.28905786, -0.28788562, -0.35557432,  0.24871536,\n",
      "       -0.25285682, -0.28421678,  0.22087605, -0.21119114, -0.31075996,\n",
      "       -0.30867223,  0.22865587, -0.27193458, -0.27452195,  0.25154231,\n",
      "       -0.360527  ,  0.22455827,  0.20665627, -0.43527902, -0.26946155,\n",
      "       -0.36150632,  0.19618378, -0.30094371, -0.31228308, -0.3424745 ,\n",
      "       -0.26256907,  0.19298289,  0.25958147, -0.33112201,  0.17940168,\n",
      "        0.18079864,  0.21232929, -0.2625592 , -0.29788975, -0.40590691,\n",
      "       -0.26355419,  0.15850784, -0.29596641,  0.33900612, -0.27794048,\n",
      "        0.21376127,  0.21810609,  0.19599818, -0.32314956, -0.2958278 ,\n",
      "       -0.29804596, -0.28499115, -0.28295926, -0.41069261, -0.27604864,\n",
      "        0.26075399,  0.20364967, -0.43127725, -0.25790483, -0.3132119 ,\n",
      "        0.19330193, -0.35124207, -0.29129499, -0.26970801, -0.25352789,\n",
      "       -0.2754248 ,  0.21583381, -0.26803212, -0.39964182,  0.1730799 ,\n",
      "        0.18926231,  0.20941375,  0.20750484, -0.32096258, -0.27196529,\n",
      "       -0.31538927,  0.26913418, -0.30465436, -0.25848521, -0.35469719,\n",
      "        0.19547753,  0.19638307,  0.22521509,  0.30157152, -0.31533849,\n",
      "       -0.27721409, -0.28620834, -0.26926805, -0.31732899, -0.28347353,\n",
      "        0.2020234 , -0.20452498, -0.30630767,  0.27537002,  0.29518446,\n",
      "        0.20476795, -0.29519766, -0.30600962, -0.29139341, -0.27241494,\n",
      "        0.19723706, -0.30128535, -0.28875476, -0.33441409, -0.30683721,\n",
      "       -0.28342741,  0.22803182, -0.17542555,  0.25561344,  0.20295379,\n",
      "       -0.27481298,  0.17357631,  0.25977697, -0.24526071, -0.29051308,\n",
      "        0.22532471, -0.36466292, -0.36067937,  0.24471292, -0.23727436,\n",
      "       -0.24936885,  0.34955817, -0.31745356, -0.27025717,  0.20361463,\n",
      "        0.28703274,  0.19968929, -0.21241455,  0.16248319, -0.35428364,\n",
      "        0.23291347, -0.2702787 , -0.31043411,  0.20246097,  0.20643516,\n",
      "       -0.24887453,  0.19004296, -0.38444981, -0.31480186,  0.2848855 ])}\n",
      "resetting env. episode reward total was -11.000000. running mean: -16.829706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -13.000000. running mean: -16.791409\n",
      "resetting env. episode reward total was -16.000000. running mean: -16.783495\n",
      "resetting env. episode reward total was -13.000000. running mean: -16.745660\n",
      "resetting env. episode reward total was -16.000000. running mean: -16.738203\n",
      "resetting env. episode reward total was -16.000000. running mean: -16.730821\n",
      "resetting env. episode reward total was -15.000000. running mean: -16.713513\n",
      "resetting env. episode reward total was -18.000000. running mean: -16.726378\n",
      "resetting env. episode reward total was -13.000000. running mean: -16.689114\n",
      "resetting env. episode reward total was -11.000000. running mean: -16.632223\n",
      "22222222222222222222222222\n",
      "{'1': array([[ 0.00987915,  0.01730566, -0.0104785 , ...,  0.05428426,\n",
      "         0.11652687, -0.00202735],\n",
      "       [-0.01013475, -0.00040148,  0.01210269, ...,  0.04497447,\n",
      "         0.02373161,  0.006018  ],\n",
      "       [-0.01541359,  0.03231854,  0.00090975, ...,  0.04952917,\n",
      "         0.05258451, -0.0077086 ],\n",
      "       ..., \n",
      "       [ 0.02556545, -0.01513545, -0.01541409, ...,  0.07481852,\n",
      "         0.06989624, -0.00118286],\n",
      "       [ 0.01317162,  0.00565197,  0.01091029, ...,  0.07959058,\n",
      "         0.07163388,  0.00830596],\n",
      "       [ 0.01149259, -0.03073655, -0.01425878, ...,  0.04378024,\n",
      "         0.13966429,  0.00165252]]), '2': array([ 0.19932273, -0.19619556, -0.24933047,  0.21877283,  0.26067811,\n",
      "        0.23116806,  0.28889362,  0.1743788 , -0.27590332, -0.26174174,\n",
      "       -0.30465518, -0.28828418,  0.22472156,  0.27313116, -0.28098854,\n",
      "        0.19822632,  0.22487537, -0.2787138 , -0.29196743,  0.19053812,\n",
      "        0.2099689 , -0.26284766,  0.23410932, -0.27464239, -0.27958516,\n",
      "       -0.2370656 ,  0.19910357,  0.16919648, -0.35497227, -0.30128251,\n",
      "       -0.25072487, -0.26576247, -0.33990044, -0.28575558, -0.3511949 ,\n",
      "        0.23314177,  0.20527143, -0.28206369,  0.2615862 ,  0.25840042,\n",
      "        0.30753371, -0.27501603,  0.25444154, -0.32594591,  0.21931303,\n",
      "       -0.33297411, -0.24892734,  0.37781299,  0.21288532,  0.1779324 ,\n",
      "        0.23870325,  0.32037148, -0.3067816 , -0.23781409, -0.27257791,\n",
      "        0.25214267, -0.25054647,  0.20351296,  0.19408882, -0.39657725,\n",
      "       -0.31193941, -0.27680373,  0.26847443,  0.24698792,  0.19765016,\n",
      "        0.27794834, -0.28895627, -0.28754725, -0.35510268,  0.24910266,\n",
      "       -0.25226536, -0.28397034,  0.22114244, -0.21082586, -0.31052188,\n",
      "       -0.30839611,  0.22899284, -0.2715575 , -0.27426779,  0.25189261,\n",
      "       -0.36032449,  0.22498082,  0.20708469, -0.43487679, -0.26923405,\n",
      "       -0.36111208,  0.19633925, -0.30047298, -0.31172628, -0.3418493 ,\n",
      "       -0.26213409,  0.19347966,  0.25985978, -0.33080145,  0.17980523,\n",
      "        0.18112766,  0.21277045, -0.26196998, -0.29755406, -0.40581747,\n",
      "       -0.26320566,  0.15866092, -0.29573351,  0.33942337, -0.277648  ,\n",
      "        0.21401115,  0.21845806,  0.19645757, -0.32280089, -0.29559811,\n",
      "       -0.29753598, -0.28458378, -0.28302633, -0.41019868, -0.27557508,\n",
      "        0.26114418,  0.20407839, -0.43074472, -0.25759948, -0.31282288,\n",
      "        0.19365355, -0.35069206, -0.29086854, -0.26940574, -0.25316117,\n",
      "       -0.27495463,  0.21631413, -0.26777277, -0.39936263,  0.17346285,\n",
      "        0.18973026,  0.20962247,  0.20785211, -0.32029531, -0.27133708,\n",
      "       -0.31488713,  0.26955073, -0.30426019, -0.258136  , -0.35448784,\n",
      "        0.19579739,  0.19671624,  0.22564699,  0.30180708, -0.31500313,\n",
      "       -0.27674028, -0.2861659 , -0.26900689, -0.31700041, -0.28302522,\n",
      "        0.20241982, -0.20409146, -0.30604501,  0.27576182,  0.29544511,\n",
      "        0.20520411, -0.29509257, -0.30584429, -0.29122459, -0.27204299,\n",
      "        0.19738057, -0.30117723, -0.28830439, -0.33401165, -0.30685213,\n",
      "       -0.28322354,  0.22851997, -0.17539454,  0.255789  ,  0.20338101,\n",
      "       -0.27466874,  0.17396501,  0.26006311, -0.2449523 , -0.2900885 ,\n",
      "        0.22567263, -0.36445842, -0.36061958,  0.245294  , -0.23726686,\n",
      "       -0.24912186,  0.34990802, -0.31710443, -0.27028591,  0.20403678,\n",
      "        0.2872868 ,  0.20008889, -0.212384  ,  0.16274129, -0.35394846,\n",
      "        0.23317618, -0.26994802, -0.3103474 ,  0.20284802,  0.20687463,\n",
      "       -0.24845073,  0.19043678, -0.3842224 , -0.31410226,  0.28519114])}\n",
      "resetting env. episode reward total was -19.000000. running mean: -16.655901\n",
      "resetting env. episode reward total was -15.000000. running mean: -16.639342\n",
      "resetting env. episode reward total was -15.000000. running mean: -16.622948\n",
      "resetting env. episode reward total was -17.000000. running mean: -16.626719\n",
      "resetting env. episode reward total was -16.000000. running mean: -16.620451\n",
      "resetting env. episode reward total was -15.000000. running mean: -16.604247\n",
      "resetting env. episode reward total was -12.000000. running mean: -16.558204\n",
      "resetting env. episode reward total was -10.000000. running mean: -16.492622\n",
      "resetting env. episode reward total was -11.000000. running mean: -16.437696\n",
      "resetting env. episode reward total was -1.000000. running mean: -16.283319\n",
      "22222222222222222222222222\n",
      "{'1': array([[ 0.00987915,  0.01730566, -0.0104785 , ...,  0.05423843,\n",
      "         0.11643011, -0.00202735],\n",
      "       [-0.01013475, -0.00040148,  0.01210269, ...,  0.04497482,\n",
      "         0.02301709,  0.006018  ],\n",
      "       [-0.01541359,  0.03231854,  0.00090975, ...,  0.04952961,\n",
      "         0.05176609, -0.0077086 ],\n",
      "       ..., \n",
      "       [ 0.02556545, -0.01513545, -0.01541409, ...,  0.07481919,\n",
      "         0.06915188, -0.00118286],\n",
      "       [ 0.01317162,  0.00565197,  0.01091029, ...,  0.07959113,\n",
      "         0.0709057 ,  0.00830596],\n",
      "       [ 0.01149259, -0.03073655, -0.01425878, ...,  0.04371477,\n",
      "         0.13954201,  0.00165252]]), '2': array([ 0.199397  , -0.19619646, -0.2493026 ,  0.21896626,  0.26083866,\n",
      "        0.2312669 ,  0.28905181,  0.17464873, -0.27575379, -0.26151941,\n",
      "       -0.30441014, -0.28804421,  0.22492147,  0.27327723, -0.28125351,\n",
      "        0.19836368,  0.22516387, -0.27860218, -0.29225118,  0.19068295,\n",
      "        0.21014932, -0.26295778,  0.23419031, -0.27461487, -0.27945057,\n",
      "       -0.23755315,  0.19932813,  0.16933618, -0.35501966, -0.30129565,\n",
      "       -0.25094547, -0.26567604, -0.33944298, -0.28566078, -0.35132466,\n",
      "        0.23336851,  0.20532628, -0.28198312,  0.26177219,  0.25850988,\n",
      "        0.30784716, -0.27491199,  0.25462517, -0.32586337,  0.21941335,\n",
      "       -0.33267698, -0.24895941,  0.37813931,  0.21303791,  0.17812519,\n",
      "        0.23886127,  0.32054094, -0.30663591, -0.23794017, -0.27259122,\n",
      "        0.25236242, -0.25036675,  0.20367376,  0.19436236, -0.39654451,\n",
      "       -0.31169777, -0.27673766,  0.26869317,  0.2471527 ,  0.19786397,\n",
      "        0.27813214, -0.28880498, -0.28747455, -0.35514217,  0.24930686,\n",
      "       -0.25237922, -0.28412038,  0.22133775, -0.21074431, -0.31029661,\n",
      "       -0.308585  ,  0.22929801, -0.27159761, -0.27423628,  0.25207679,\n",
      "       -0.36033815,  0.22523342,  0.20724804, -0.43498995, -0.26920314,\n",
      "       -0.36098215,  0.19663219, -0.30045261, -0.31194683, -0.34160617,\n",
      "       -0.26221772,  0.19368979,  0.26003793, -0.33048438,  0.17993945,\n",
      "        0.18138495,  0.21280536, -0.26173629, -0.29766059, -0.40609813,\n",
      "       -0.26320455,  0.15874138, -0.29559186,  0.33954623, -0.27757504,\n",
      "        0.2142069 ,  0.21870395,  0.19662551, -0.32292982, -0.2954916 ,\n",
      "       -0.2975163 , -0.28459114, -0.28295809, -0.41010538, -0.27560857,\n",
      "        0.26146173,  0.20429573, -0.43068323, -0.25734692, -0.3130064 ,\n",
      "        0.19395521, -0.35082549, -0.29081095, -0.26943658, -0.25330385,\n",
      "       -0.27512366,  0.21658045, -0.26782346, -0.39934072,  0.17367119,\n",
      "        0.18969833,  0.20970376,  0.20808362, -0.32017332, -0.27100714,\n",
      "       -0.31479901,  0.26967677, -0.30393256, -0.25806118, -0.35425758,\n",
      "        0.19601395,  0.197047  ,  0.22578103,  0.30199278, -0.31459568,\n",
      "       -0.27681048, -0.28592861, -0.26888559, -0.31707308, -0.28294316,\n",
      "        0.20267756, -0.20413212, -0.30608359,  0.27580093,  0.29563588,\n",
      "        0.20534116, -0.29499595, -0.30588972, -0.29150206, -0.27180564,\n",
      "        0.19766345, -0.30116042, -0.28840595, -0.33386249, -0.307116  ,\n",
      "       -0.28310203,  0.22866373, -0.17551559,  0.25592213,  0.20359135,\n",
      "       -0.27475499,  0.17413374,  0.26015315, -0.24507855, -0.29010466,\n",
      "        0.22578589, -0.36416576, -0.360765  ,  0.2454191 , -0.2374454 ,\n",
      "       -0.24902235,  0.35023516, -0.31673262, -0.2705149 ,  0.20422451,\n",
      "        0.28749056,  0.20029137, -0.21265018,  0.1629346 , -0.3533525 ,\n",
      "        0.23353388, -0.27015665, -0.31014189,  0.20319754,  0.20710013,\n",
      "       -0.24838979,  0.19071419, -0.38435741, -0.31397244,  0.28548686])}\n",
      "resetting env. episode reward total was -14.000000. running mean: -16.260486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -7.000000. running mean: -16.167881\n",
      "resetting env. episode reward total was -15.000000. running mean: -16.156202\n",
      "resetting env. episode reward total was -12.000000. running mean: -16.114640\n"
     ]
    }
   ],
   "source": [
    "## Architecture\n",
    "\n",
    "# Take in inputs from the screen and preprocess them\n",
    "# Pass them into an NN\n",
    "# Update the weights of the NN using gradient descent\n",
    "# weights['1'] - Matrix that holds weights of pixels passing into hidden layer. Dimensions: [200 x 80 x 80] -> [200 x 6400]\n",
    "# weights['2'] - Matrix that holds weights of hidden layer passing into output. Dimensions: [1 x 200]\n",
    "\n",
    "# Process is:\n",
    "\n",
    "# processed_observations = image vector - [6400 x 1] array\n",
    "# Compute hidden_layer_values = weights['1'] dot processed_observations ([200 x 6400] dot [6400 x 1]) -> [200 x 1] - this gives initial activation values.\n",
    "# Next we need to transform those either via a sigmoid or an ReLU of some sort. Let's use ReLU\n",
    "# ReLU(hidden_layer_values)\n",
    "# Next we need to pass this one layer further\n",
    "# output_layer_value = weights['2'] dot hidden_layer_values ([1 x 200] dot [200 x 1] -> [1 x 1])\n",
    "# Now our output layer is the probability of going up or down. Let's make sure this output is between 0 and 1 by passing it through a sigmoid\n",
    "# p = sigmoid(output_layer_value)\n",
    "\n",
    "# Learning after round has finished:\n",
    "\n",
    "# Figure out the result\n",
    "# Compute the error\n",
    "# Use the error to calculate the gradient\n",
    "    # The below dimensions all assume we had exactly 10 frames in the round (not necessarily true!)\n",
    "    # dC_dw2 = hidden_layer_values^T dot gradient_log_p ([1 x 2000] dot [2000 x 1] -> 1x1)\n",
    "    # delta_1 = gradient_log_p outer_product weights['2'] = [2000 x 1] outer_product [1 x 200] ([2000 x 200])\n",
    "    # dC_dw1 = delta_1^T dot input_observations ([200 x 2000]x dot [2000 x 64000] -> [200 x 64000])\n",
    "\n",
    "# After some batch size of rounds has finished,\n",
    "    # Use rmsprop to move weights['1'] and weights['2'] in the direction of the gradient\n",
    "# Repeat!\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def downsample(image):\n",
    "    # Take only alternate pixels - basically halves the resolution of the image (which is fine for us)\n",
    "    return image[::2, ::2, :]#read a='hello' a[::2] = 'hlo'\n",
    "\n",
    "def remove_color(image):\n",
    "    \"\"\"Convert all color (RGB is the third dimension in the image)\"\"\"\n",
    "    return image[:, :, 0]\n",
    "\n",
    "def remove_background(image):\n",
    "    image[image == 144] = 0 #remove the number 144,list is not run , and np.array is ok a = np.array[12,23,144] result is a = [12,23,0]\n",
    "    image[image == 109] = 0\n",
    "    return image\n",
    "\n",
    "def preprocess_observations(input_observation, prev_processed_observation, input_dimensions):\n",
    "    \"\"\" convert the 210x160x3 uint8 frame into a 6400 float vector \"\"\"\n",
    "    processed_observation = input_observation[35:195] # crop\n",
    "    processed_observation = downsample(processed_observation)\n",
    "    processed_observation = remove_color(processed_observation)\n",
    "    processed_observation = remove_background(processed_observation)\n",
    "    processed_observation[processed_observation != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    # Convert from 80 x 80 matrix to 1600 x 1 matrix\n",
    "    processed_observation = processed_observation.astype(np.float).ravel()\n",
    "\n",
    "    # subtract the previous frame from the current one so we are only processing on changes in the game\n",
    "    if prev_processed_observation is not None:\n",
    "        input_observation = processed_observation - prev_processed_observation\n",
    "    else:\n",
    "        input_observation = np.zeros(input_dimensions)\n",
    "    # store the previous frame so we can subtract from it next time\n",
    "    prev_processed_observations = processed_observation\n",
    "    return input_observation, prev_processed_observations\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def relu(vector):\n",
    "    vector[vector < 0] = 0\n",
    "    return vector\n",
    "\n",
    "def apply_neural_nets(observation_matrix, weights):\n",
    "    \"\"\" Based on the observation_matrix and weights, compute the new hidden layer values and the new output layer values\"\"\"\n",
    "    hidden_layer_values = np.dot(weights['1'], observation_matrix)\n",
    "    hidden_layer_values = relu(hidden_layer_values)\n",
    "    output_layer_values = np.dot(hidden_layer_values, weights['2'])\n",
    "    output_layer_values = sigmoid(output_layer_values)\n",
    "    return hidden_layer_values, output_layer_values\n",
    "\n",
    "def choose_action(probability):\n",
    "    random_value = np.random.uniform()\n",
    "    if random_value < probability:\n",
    "        # signifies up in openai gym\n",
    "        return 2\n",
    "    else:\n",
    "         # signifies down in openai gym\n",
    "        return 3\n",
    "\n",
    "def compute_gradient(gradient_log_p, hidden_layer_values, observation_values, weights):\n",
    "    \"\"\" See here: http://neuralnetworksanddeeplearning.com/chap2.html\"\"\"\n",
    "    delta_L = gradient_log_p\n",
    "    dC_dw2 = np.dot(hidden_layer_values.T, delta_L).ravel()\n",
    "    delta_l2 = np.outer(delta_L, weights['2'])\n",
    "    delta_l2 = relu(delta_l2)\n",
    "    dC_dw1 = np.dot(delta_l2.T, observation_values)\n",
    "    return {\n",
    "        '1': dC_dw1,\n",
    "        '2': dC_dw2\n",
    "    }\n",
    "\n",
    "def update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate):\n",
    "    \"\"\" See here: http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop\"\"\"\n",
    "    epsilon = 1e-5\n",
    "    for layer_name in weights.keys():\n",
    "        g = g_dict[layer_name]\n",
    "        expectation_g_squared[layer_name] = decay_rate * expectation_g_squared[layer_name] + (1 - decay_rate) * g**2\n",
    "        weights[layer_name] += (learning_rate * g)/(np.sqrt(expectation_g_squared[layer_name] + epsilon))\n",
    "        g_dict[layer_name] = np.zeros_like(weights[layer_name]) # reset batch gradient buffer\n",
    "        \n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "    \"\"\" Actions you took 20 steps before the end result are less important to the overall result than an action you took a step ago.\n",
    "    This implements that logic by discounting the reward on previous actions based on how long ago they were taken\"\"\"\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, rewards.size)):\n",
    "        if rewards[t] != 0:\n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_with_rewards(gradient_log_p, episode_rewards, gamma):\n",
    "    \"\"\" discount the gradient with the normalized rewards \"\"\"\n",
    "    discounted_episode_rewards = discount_rewards(episode_rewards, gamma)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "    discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
    "    return gradient_log_p * discounted_episode_rewards\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    env.mode = 'fast'\n",
    "    observation = env.reset() # This gets us the image\n",
    "\n",
    "    # hyperparameters\n",
    "    episode_number = 0\n",
    "    batch_size = 10\n",
    "    gamma = 0.99 # discount factor for reward\n",
    "    decay_rate = 0.99\n",
    "    num_hidden_layer_neurons = 200\n",
    "    input_dimensions = 80 * 80\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    episode_number = 0\n",
    "    reward_sum = 0\n",
    "    running_reward = None\n",
    "    prev_processed_observations = None\n",
    "\n",
    "    weights = {\n",
    "        '1': np.random.randn(num_hidden_layer_neurons, input_dimensions) / np.sqrt(input_dimensions),\n",
    "        '2': np.random.randn(num_hidden_layer_neurons) / np.sqrt(num_hidden_layer_neurons)\n",
    "    }\n",
    "\n",
    "    # To be used with rmsprop algorithm (http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)\n",
    "    expectation_g_squared = {}\n",
    "    g_dict = {}\n",
    "    for layer_name in weights.keys():\n",
    "        expectation_g_squared[layer_name] = np.zeros_like(weights[layer_name])\n",
    "        g_dict[layer_name] = np.zeros_like(weights[layer_name])\n",
    "\n",
    "    episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        env.render()\n",
    "        processed_observations, prev_processed_observations = preprocess_observations(observation, prev_processed_observations, input_dimensions)\n",
    "        hidden_layer_values, up_probability = apply_neural_nets(processed_observations, weights)\n",
    "    \n",
    "        episode_observations.append(processed_observations)\n",
    "        episode_hidden_layer_values.append(hidden_layer_values)\n",
    "\n",
    "        action = choose_action(up_probability)\n",
    "\n",
    "        # carry out the chosen action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        reward_sum += reward\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        # see here: http://cs231n.github.io/neural-networks-2/#losses\n",
    "        fake_label = 1 if action == 2 else 0\n",
    "        loss_function_gradient = fake_label - up_probability\n",
    "        episode_gradient_log_ps.append(loss_function_gradient)\n",
    "\n",
    "\n",
    "        if done: # an episode finished\n",
    "            episode_number += 1\n",
    "\n",
    "            # Combine the following values for the episode\n",
    "            episode_hidden_layer_values = np.vstack(episode_hidden_layer_values)\n",
    "            episode_observations = np.vstack(episode_observations)\n",
    "            episode_gradient_log_ps = np.vstack(episode_gradient_log_ps)\n",
    "            episode_rewards = np.vstack(episode_rewards)\n",
    "\n",
    "            # Tweak the gradient of the log_ps based on the discounted rewards\n",
    "            episode_gradient_log_ps_discounted = discount_with_rewards(episode_gradient_log_ps, episode_rewards, gamma)\n",
    "\n",
    "            gradient = compute_gradient(\n",
    "              episode_gradient_log_ps_discounted,\n",
    "              episode_hidden_layer_values,\n",
    "              episode_observations,\n",
    "              weights\n",
    "            )\n",
    "\n",
    "            # Sum the gradient for use when we hit the batch size\n",
    "            for layer_name in gradient:\n",
    "                g_dict[layer_name] += gradient[layer_name]\n",
    "\n",
    "            if episode_number % batch_size == 0:\n",
    "                update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate)\n",
    "                \n",
    "                if i==1:\n",
    "                    with open('file1.txt', 'rb') as handle:\n",
    "                        weights = pickle.loads(handle.read())\n",
    "                        i = i + 1\n",
    "                    \n",
    "                    print \"1111111111111111111111111\"\n",
    "                    print weights\n",
    "                    \n",
    "                else:\n",
    "                    with open('file1.txt', 'wb') as handle:\n",
    "                        pickle.dump(weights, handle)\n",
    "                    i = i + 1\n",
    "                    \n",
    "                    print \"22222222222222222222222222\"\n",
    "                    print weights\n",
    "\n",
    "\n",
    "            episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], [] # reset values\n",
    "            observation = env.reset() # reset env\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print 'resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward)\n",
    "            reward_sum = 0\n",
    "            prev_processed_observations = None\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
